{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks with PyTorch\n",
        "\n",
        "In this assignment, we are going to train a Neural Networks on the Japanese MNIST dataset. It is composed of 70000 images of handwritten Hiragana characters. The target variables has 10 different classes.\n",
        "\n",
        "Each image is of dimension 28 by 28. But we will flatten them to form a dataset composed of vectors of dimension (784, 1). The training process will be similar as for a structured dataset.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=16TqEl9ESfXYbUpVafXD6h5UpJYGKfMxE' width=\"500\" height=\"200\">\n",
        "\n",
        "Your goal is to run at least 3 experiments and get a model that can achieve 80% accuracy with not much overfitting on this dataset.\n",
        "\n",
        "Some of the code have already been defined for you. You need only to add your code in the sections specified (marked with **TODO**). Some assert statements have been added to verify the expected outputs are correct. If it does throw an error, this means your implementation is behaving as expected.\n",
        "\n",
        "Note: You can only use fully-connected and dropout layers for this assignment. You can not convolution layers for instance"
      ],
      "metadata": {
        "id": "KNyZ-zZxlU6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Required Packages"
      ],
      "metadata": {
        "id": "iOufKqO8mw7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1.1] We are going to use numpy, matplotlib and google.colab packages"
      ],
      "metadata": {
        "id": "b-sGJ26pmz4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTGG80etnMAa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Download Dataset\n",
        "\n",
        "We will store the dataset into your personal Google Drive.\n"
      ],
      "metadata": {
        "id": "Vyky0K3fnEFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.1] Mount Google Drive"
      ],
      "metadata": {
        "id": "ltUMtjG-nX-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "N_FVrXICnMJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.2] Create a folder called `DL_ASG_1` on your Google Drive at the root level"
      ],
      "metadata": {
        "id": "CzLtlKCHnT9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p /content/gdrive/MyDrive/DL_ASG_1"
      ],
      "metadata": {
        "id": "XZicoPks4POW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.3] Navigate to this folder"
      ],
      "metadata": {
        "id": "sToej_3CnePP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/gdrive/MyDrive/DL_ASG_1'"
      ],
      "metadata": {
        "id": "g2oAXToKnpXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.4] Show the list of item on the folder"
      ],
      "metadata": {
        "id": "TnRHIyhlzUwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "Y-xYtezBzQ0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.4] Dowload the dataset files to your Google Drive if required"
      ],
      "metadata": {
        "id": "3vlfobqnnjJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import os.path\n",
        "\n",
        "def download_file(url):\n",
        "    path = url.split('/')[-1]\n",
        "    if os.path.isfile(path):\n",
        "        print (f\"{path} already exists\")\n",
        "    else:\n",
        "      r = requests.get(url, stream=True)\n",
        "      with open(path, 'wb') as f:\n",
        "          total_length = int(r.headers.get('content-length'))\n",
        "          print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
        "          for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
        "              if chunk:\n",
        "                  f.write(chunk)\n",
        "\n",
        "url_list = [\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'\n",
        "]\n",
        "\n",
        "for url in url_list:\n",
        "    download_file(url)"
      ],
      "metadata": {
        "id": "M0owzTC427NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.5] List the content of the folder and confirm files have been dowloaded properly"
      ],
      "metadata": {
        "id": "DVF_Cx7Hny2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "vt6ZKf4fnqkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load Data"
      ],
      "metadata": {
        "id": "fvvfOON36hTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.1] Import the required modules from PyTorch"
      ],
      "metadata": {
        "id": "duFjgsyPoLPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "1zolHKEO7GZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.2] **TODO** Create 2 variables called `img_height` and `img_width` that will both take the value 28"
      ],
      "metadata": {
        "id": "r4Aw5ObQoWdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "img_height = 28\n",
        "img_width = 28"
      ],
      "metadata": {
        "id": "Ip0NFeyjpj79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.3] Create a function that loads a .npz file using numpy and return the content of the `arr_0` key"
      ],
      "metadata": {
        "id": "hmX5SEHkpp63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load(f):\n",
        "    return np.load(f)['arr_0']"
      ],
      "metadata": {
        "id": "5S3cthx57L2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.4] **TODO** Load the 4 files saved on your Google Drive into their respective variables: x_train, y_train, x_test and y_test"
      ],
      "metadata": {
        "id": "8V2Ij9s7qRtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "x_train = load('kmnist-train-imgs.npz')\n",
        "x_test = load('kmnist-test-imgs.npz')\n",
        "y_train = load('kmnist-train-labels.npz')\n",
        "y_test = load('kmnist-test-labels.npz')\n"
      ],
      "metadata": {
        "id": "5XTkRb0lqpEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.5] **TODO** Using matplotlib display the first image from the train set and its target value"
      ],
      "metadata": {
        "id": "3KC12nB7rlbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "plt.imshow(x_train[0])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "AOtWg7bBrwmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Prepare Data"
      ],
      "metadata": {
        "id": "htLk_27ir0B1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.1] **TODO** Reshape the images from the training and testing set to have the channel dimension last. The dimensions should be: (row_number, height, width, channel)"
      ],
      "metadata": {
        "id": "VJEBe30Er33P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "x_train = x_train.reshape(x_train.shape[0], img_height, img_width, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_height, img_width, 1)"
      ],
      "metadata": {
        "id": "1yqWleZasxdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")"
      ],
      "metadata": {
        "id": "MLl1YM7A8_4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.2] **TODO** Cast `x_train` and `x_test` into `float32` decimals"
      ],
      "metadata": {
        "id": "F2f6wvFys2ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)"
      ],
      "metadata": {
        "id": "FWZmWe73tLXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.3] **TODO** Standardise the images of the training and testing sets. Originally each image contains pixels with value ranging from 0 to 255. after standardisation, the new value range should be from 0 to 1."
      ],
      "metadata": {
        "id": "Z-1Jr0pKs6jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n"
      ],
      "metadata": {
        "id": "RXY1o272t0JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Min value in x_train: {x_train.min()}, Max value in x_train: {x_train.max()}\")\n",
        "print(f\"Min value in x_test: {x_test.min()}, Max value in x_test: {x_test.max()}\")"
      ],
      "metadata": {
        "id": "WqyNBvct9KgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "print(x_train[0][0].shape)"
      ],
      "metadata": {
        "id": "EEqyY5m2DFEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.4] **TODO** Create a variable called `num_classes` that will take the value 10 which corresponds to the number of classes for the target variable"
      ],
      "metadata": {
        "id": "9eH4aZmXt7Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "num_classes = 10"
      ],
      "metadata": {
        "id": "gTnMgLxYuUs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.5] **TODO** Convert the target variable for the training and testing sets to a binary class matrix of dimension (rows, num_classes).\n",
        "\n",
        "For example:\n",
        "- class 0 will become [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "- class 1 will become [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "- class 5 will become [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "- class 9 will become [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
      ],
      "metadata": {
        "id": "iAy0fUJsuyhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"Unique values in y_train:\", np.unique(y_train))\n",
        "print(\"Unique values in y_test:\", np.unique(y_test))"
      ],
      "metadata": {
        "id": "QR76eXZtG6xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "4PNeP6VoHp9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "id": "Bns37G6ciWIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0]"
      ],
      "metadata": {
        "id": "QJBmU-oDiOE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Data into Train and Val"
      ],
      "metadata": {
        "id": "eunf7pE2Y_Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n"
      ],
      "metadata": {
        "id": "68BL87ZxZDw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Experiment 1"
      ],
      "metadata": {
        "id": "9PstbPkN9re_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define Neural Networks Architecure"
      ],
      "metadata": {
        "id": "0OCorS00wxPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.1] Set the seed in PyTorch for reproducing results\n",
        "\n"
      ],
      "metadata": {
        "id": "7G_L-yqTxI1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "torch.manual_seed(21)"
      ],
      "metadata": {
        "id": "XB8OIC9wrgFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Define the architecture of your Neural Networks and save it into a variable called `model`"
      ],
      "metadata": {
        "id": "5b93U4MixWeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "x9GqGrrfVobA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "layer_1 = nn.Linear(784, 512)\n",
        "layer_2 = nn.Linear(512, 512)\n",
        "layer_top = nn.Linear(512, 10)"
      ],
      "metadata": {
        "id": "gq1f74uKxpkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    layer_1,\n",
        "    nn.ReLU(),\n",
        "    layer_2,\n",
        "    nn.ReLU(),\n",
        "    layer_top\n",
        ")"
      ],
      "metadata": {
        "id": "gQ3-KgYWVwIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Print the summary of your model"
      ],
      "metadata": {
        "id": "0IvuMQ81xu5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "print(model1)\n"
      ],
      "metadata": {
        "id": "gBRm-h5dxvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.to(device)"
      ],
      "metadata": {
        "id": "in5EpT--i4Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train Neural Networks"
      ],
      "metadata": {
        "id": "sOPTnNxtx6MC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.1] **TODO** Create 2 variables called `batch_size` and `epochs` that will  respectively take the values 128 and 500"
      ],
      "metadata": {
        "id": "fsHJzhnAyP4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "batch_size = 128\n",
        "epochs = 500"
      ],
      "metadata": {
        "id": "Tq-bftwDXnhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.2] **TODO** Compile your model with the appropriate loss function, the optimiser of your choice and the accuracy metric"
      ],
      "metadata": {
        "id": "4-bAkzwXyjAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "FvaKXV4qV_wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model1.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "GfYralfrWNgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Detect GPU (CUDA) or fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors and move to the correct device\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "\n",
        "x_val_tensor = torch.tensor(x_val, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
        "\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "# Define batch size\n",
        "BATCH_SIZE = batch_size  # Ensure batch_size is properly defined\n",
        "\n",
        "# Create DataLoaders\n",
        "dataloader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_val = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "dataloader_test = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Check the number of batches\n",
        "print(f\"Train batches: {len(dataloader_train)}\")\n",
        "print(f\"Validation batches: {len(dataloader_val)}\")\n",
        "print(f\"Test batches: {len(dataloader_test)}\")\n"
      ],
      "metadata": {
        "id": "onXZ4jQ2ENxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.3] **TODO** Train your model\n",
        "using the number of epochs defined. Calculate the total loss and save it to a variable called total_loss."
      ],
      "metadata": {
        "id": "iRvM_pEZy7SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = epochs\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model1.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for x_batch, y_batch in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model1(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_losses.append(train_loss / len(dataloader_train))\n",
        "\n",
        "    model1.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in dataloader_val:\n",
        "            val_outputs = model1(x_val)\n",
        "            val_loss += criterion(val_outputs, y_val).item()\n",
        "\n",
        "    val_losses.append(val_loss / len(dataloader_val))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "FC7Zq41fTqxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_loss = sum(train_losses)\n",
        "final_train_loss = train_losses[-1]\n",
        "\n",
        "total_val_loss = sum(val_losses)\n",
        "final_val_loss = val_losses[-1]\n",
        "\n",
        "print(f\"Total Training Loss: {total_train_loss:.4f}\")\n",
        "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Total Validation Loss: {total_val_loss:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "9Wf6cnG4kiIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.4] **TODO** Test your model.  Initiate the model.eval() along with torch.no_grad() to turn off the gradients.\n"
      ],
      "metadata": {
        "id": "emZ5Ayr88PZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "\n",
        "model1.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "total_test_loss = 0\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in dataloader_test:\n",
        "        data = data.view(-1, 28*28).to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        if target.ndim > 1:\n",
        "            target = target.argmax(dim=1)\n",
        "\n",
        "        outputs = model1(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        predicted_labels.extend(predicted.cpu().tolist())\n",
        "        true_labels.extend(target.cpu().tolist())\n",
        "\n",
        "avg_test_loss = total_test_loss / len(dataloader_test)\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "j8eMpGAUfKjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device, dataset_type=\"test\"):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    predicted_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data = data.view(-1, 28*28).to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            if target.ndim > 1:\n",
        "                target = target.argmax(dim=1)\n",
        "\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            predicted_labels.extend(predicted.cpu().tolist())\n",
        "            true_labels.extend(target.cpu().tolist())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"{dataset_type.capitalize()} Loss: {avg_loss:.4f}\")\n",
        "    print(f\"{dataset_type.capitalize()} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "9wa7BfwY0VVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_accuracy = evaluate_model(model1, dataloader_val, criterion, device, dataset_type=\"val\")\n"
      ],
      "metadata": {
        "id": "Z5lbZz4a0YKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_accuracy = evaluate_model(model1, dataloader_train, criterion, device, dataset_type=\"train\")\n"
      ],
      "metadata": {
        "id": "wCJ7UvIF0ZU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Analyse Results"
      ],
      "metadata": {
        "id": "vz9uFy_X6oeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.1] **TODO** Display the performance of your model on the training and testing sets"
      ],
      "metadata": {
        "id": "ddugPZhZ68Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (outputs[0])"
      ],
      "metadata": {
        "id": "qqEobkFYfoWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = np.argmax(outputs.cpu().numpy(), axis=1)\n",
        "pred_test[0]"
      ],
      "metadata": {
        "id": "AmhEvu_Hfvgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target[0]"
      ],
      "metadata": {
        "id": "_lq_jPJNf1AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate_model(model1, dataloader_test, criterion, device, dataset_type=\"test\")\n"
      ],
      "metadata": {
        "id": "ys2o-Bmxf7hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "bZPqsj306Utm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.2] **TODO** Plot the learning curve of your model"
      ],
      "metadata": {
        "id": "iBTo_xEI7K_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Train vs Validation Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jRt_4W2F7RVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.3] **TODO** Display the confusion matrix on the testing set predictions"
      ],
      "metadata": {
        "id": "qKPu98GR7a17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TkrP9JCgMzpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2"
      ],
      "metadata": {
        "id": "8Riiw2Vx956R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define Neural Networks Architecure"
      ],
      "metadata": {
        "id": "5A8CfyCz-Bbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.1] Set the seed in PyTorch for reproducing results\n",
        "\n"
      ],
      "metadata": {
        "id": "9cL1Q4aL-Bbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "torch.manual_seed(44)"
      ],
      "metadata": {
        "id": "A9FdcZfd-Bbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Define the architecture of your Neural Networks and save it into a variable called `model`"
      ],
      "metadata": {
        "id": "QtCGsFT4-Bby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "layer_1 = nn.Linear(784, 512)\n",
        "layer_2 = nn.Linear(512, 512)\n",
        "layer_top = nn.Linear(512, 10)"
      ],
      "metadata": {
        "id": "7zWO5N7q-Bbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    layer_1,\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.3),\n",
        "    layer_2,\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.3),\n",
        "    layer_top\n",
        ")"
      ],
      "metadata": {
        "id": "CC31pMJE-Bbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Print the summary of your model"
      ],
      "metadata": {
        "id": "fp2frDks-Bbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "print(model2)\n"
      ],
      "metadata": {
        "id": "mmYI4qET-Bb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.to(device)"
      ],
      "metadata": {
        "id": "rszYpaAb-Bb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train Neural Networks"
      ],
      "metadata": {
        "id": "fSyp1xfX-Bb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.1] **TODO** Create 2 variables called `batch_size` and `epochs` that will  respectively take the values 128 and 500"
      ],
      "metadata": {
        "id": "qgijZOrS-Bb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "batch_size = 128\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "3byOpp2U-Bb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.2] **TODO** Compile your model with the appropriate loss function, the optimiser of your choice and the accuracy metric"
      ],
      "metadata": {
        "id": "mhSJL6mO-Bb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "DHb_w8Vt-Bb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model2.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "yP7KF72y-Bb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.to(device)"
      ],
      "metadata": {
        "id": "nWZx5yq5-Bb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.3] **TODO** Train your model\n",
        "using the number of epochs defined. Calculate the total loss and save it to a variable called total_loss."
      ],
      "metadata": {
        "id": "sV69Fbek-Bb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = epochs\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model2.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for x_batch, y_batch in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model2(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_losses.append(train_loss / len(dataloader_train))\n",
        "\n",
        "    model2.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in dataloader_val:\n",
        "            val_outputs = model2(x_val)\n",
        "            val_loss += criterion(val_outputs, y_val).item()\n",
        "\n",
        "    val_losses.append(val_loss / len(dataloader_val))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "cDtOq6Pk-Bb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.4] **TODO** Test your model.  Initiate the model.eval() along with torch.no_grad() to turn off the gradients.\n"
      ],
      "metadata": {
        "id": "Y43kV-Fh-Bb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "\n",
        "model2.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "total_test_loss = 0\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in dataloader_test:\n",
        "        data = data.view(-1, 28*28).to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        if target.ndim > 1:\n",
        "            target = target.argmax(dim=1)\n",
        "\n",
        "        outputs = model2(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        predicted_labels.extend(predicted.cpu().tolist())\n",
        "        true_labels.extend(target.cpu().tolist())\n",
        "\n",
        "avg_test_loss = total_test_loss / len(dataloader_test)\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pzzlQZ-_-Bb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_accuracy = evaluate_model(model2, dataloader_val, criterion, device, dataset_type=\"val\")\n",
        "train_loss, train_accuracy = evaluate_model(model2, dataloader_train, criterion, device, dataset_type=\"train\")"
      ],
      "metadata": {
        "id": "K5n_PfvF3G4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Analyse Results"
      ],
      "metadata": {
        "id": "zoGvi-66-Bb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.1] **TODO** Display the performance of your model on the training and testing sets"
      ],
      "metadata": {
        "id": "kM7lBzA2-Bb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_loss = sum(train_losses)\n",
        "\n",
        "\n",
        "total_val_loss = sum(val_losses)\n",
        "\n",
        "\n",
        "print(f\"Total Training Loss: {total_train_loss:.4f}\")\n",
        "\n",
        "print(f\"Total Validation Loss: {total_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "Is804P-HkZad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (outputs[0])"
      ],
      "metadata": {
        "id": "HrXdPMUEku-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = np.argmax(outputs.cpu().numpy(), axis=1)\n",
        "pred_test[0]"
      ],
      "metadata": {
        "id": "y3refDYRku-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target[0]"
      ],
      "metadata": {
        "id": "txkVe8Etku-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate_model(model2, dataloader_test, criterion, device, dataset_type=\"test\")\n"
      ],
      "metadata": {
        "id": "EP0F_NNy4EW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.2] **TODO** Plot the learning curve of your model"
      ],
      "metadata": {
        "id": "3FD7fGCO-Bb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n",
        "plt.plot(range(1, len(train_losses) + 1), val_losses, label=\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "elV-Gu2K-Bb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.3] **TODO** Display the confusion matrix on the testing set predictions"
      ],
      "metadata": {
        "id": "vPqavtBY-Bb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "# Solution\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TuC8kfa--Bb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3\n"
      ],
      "metadata": {
        "id": "ZhpZpKoEzRcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define Neural Networks Architecure"
      ],
      "metadata": {
        "id": "WmIeXObVFckt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.1] Set the seed in PyTorch for reproducing results\n",
        "\n"
      ],
      "metadata": {
        "id": "PxJrdT-bFcku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "torch.manual_seed(46)"
      ],
      "metadata": {
        "id": "KYJqPbDgFckv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Define the architecture of your Neural Networks and save it into a variable called `model`"
      ],
      "metadata": {
        "id": "GIGRJTEKFckv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "layer_1 = nn.Linear(784, 512)\n",
        "layer_2 = nn.Linear(512, 512)\n",
        "layer_3 = nn.Linear(512, 216)\n",
        "layer_top = nn.Linear(216, 10)"
      ],
      "metadata": {
        "id": "UGMW7JBuFckw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    layer_1,\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.3),\n",
        "    layer_2,\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.3),\n",
        "    layer_3,\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.3),\n",
        "    layer_top\n",
        ")"
      ],
      "metadata": {
        "id": "ta3EZsz9Fckw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Print the summary of your model"
      ],
      "metadata": {
        "id": "o8R01_aeFckx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "print(model3)\n"
      ],
      "metadata": {
        "id": "khoi_SLrFcky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.to(device)"
      ],
      "metadata": {
        "id": "QABQwHdxFckz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train Neural Networks"
      ],
      "metadata": {
        "id": "jTN9nUoTFck1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.1] **TODO** Create 2 variables called `batch_size` and `epochs` that will  respectively take the values 128 and 500"
      ],
      "metadata": {
        "id": "KQ5Q4jQ9Fck2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "batch_size = 128\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "q1G6vtsRFck3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.2] **TODO** Compile your model with the appropriate loss function, the optimiser of your choice and the accuracy metric"
      ],
      "metadata": {
        "id": "aKJZiEzfFck4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "hzwHlH8SFck4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model3.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "DufcpZ9GFck5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.to(device)"
      ],
      "metadata": {
        "id": "18rmBlfDFck5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.3] **TODO** Train your model\n",
        "using the number of epochs defined. Calculate the total loss and save it to a variable called total_loss."
      ],
      "metadata": {
        "id": "GdVK2e8cFck6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = epochs\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model3.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for x_batch, y_batch in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model3(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_losses.append(train_loss / len(dataloader_train))\n",
        "\n",
        "    model3.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in dataloader_val:\n",
        "            val_outputs = model3(x_val)\n",
        "            val_loss += criterion(val_outputs, y_val).item()\n",
        "\n",
        "    val_losses.append(val_loss / len(dataloader_val))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "R88nBBjeFck6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.4] **TODO** Test your model.  Initiate the model.eval() along with torch.no_grad() to turn off the gradients.\n"
      ],
      "metadata": {
        "id": "cGsDViELFck7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "\n",
        "model3.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "total_test_loss = 0\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in dataloader_test:\n",
        "        data = data.view(-1, 28*28).to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        if target.ndim > 1:\n",
        "            target = target.argmax(dim=1)\n",
        "\n",
        "        outputs = model3(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        predicted_labels.extend(predicted.cpu().tolist())\n",
        "        true_labels.extend(target.cpu().tolist())\n",
        "\n",
        "avg_test_loss = total_test_loss / len(dataloader_test)\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "weS1AxfIFck7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_accuracy = evaluate_model(model3, dataloader_val, criterion, device, dataset_type=\"val\")\n"
      ],
      "metadata": {
        "id": "UdBhVRKw4cOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_accuracy = evaluate_model(model3, dataloader_train, criterion, device, dataset_type=\"train\")\n"
      ],
      "metadata": {
        "id": "LtWXGGHO4fiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Analyse Results"
      ],
      "metadata": {
        "id": "vqAvRtLOFck8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.1] **TODO** Display the performance of your model on the training and testing sets"
      ],
      "metadata": {
        "id": "MrLQXLCfFck8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_loss = sum(train_losses)\n",
        "final_train_loss = train_losses[-1]\n",
        "total_val_loss = sum(val_losses)\n",
        "final_val_loss = val_losses[-1]\n",
        "\n",
        "print(f\"Total Training Loss: {total_train_loss:.4f}\")\n",
        "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Total Validation Loss: {total_val_loss:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "g_b6SwcBpQ7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (outputs[0])"
      ],
      "metadata": {
        "id": "MQbKK-bXpQ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = np.argmax(outputs.cpu().numpy(), axis=1)\n",
        "pred_test[0]"
      ],
      "metadata": {
        "id": "7mvhxyGipQ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target[0]"
      ],
      "metadata": {
        "id": "lSvUED8apQ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "b5Bq30ADpQ7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate_model(model3, dataloader_test, criterion, device, dataset_type=\"test\")\n"
      ],
      "metadata": {
        "id": "bF315ND95rlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate_model(model3, dataloader_test, criterion, device, dataset_type=\"test\")\n"
      ],
      "metadata": {
        "id": "c7a5mv6x6-8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.2] **TODO** Plot the learning curve of your model"
      ],
      "metadata": {
        "id": "eD_uyRmnFck-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n",
        "plt.plot(range(1, len(train_losses) + 1), val_losses, label=\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K3nvutskFck-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.3] **TODO** Display the confusion matrix on the testing set predictions"
      ],
      "metadata": {
        "id": "B_ec5miqFck-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bGZndqNuFck-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 4"
      ],
      "metadata": {
        "id": "VCrdhGs6tvGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define Neural Networks Architecure"
      ],
      "metadata": {
        "id": "XFBljzO4tw-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.1] Set the seed in PyTorch for reproducing results\n",
        "\n"
      ],
      "metadata": {
        "id": "Q4vCCbDdtw-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "torch.manual_seed(6)"
      ],
      "metadata": {
        "id": "86U5NYPCtw-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Define the architecture of your Neural Networks and save it into a variable called `model`"
      ],
      "metadata": {
        "id": "gRfW3Uoqtw-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "layer_1 = nn.Linear(784, 512)\n",
        "layer_2 = nn.Linear(512, 512)\n",
        "layer_3 = nn.Linear(512, 256)\n",
        "layer_top = nn.Linear(256, 10)"
      ],
      "metadata": {
        "id": "xtnoO4DLtw-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    layer_1,\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.3),\n",
        "    layer_2,\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.3),\n",
        "    layer_3,\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.3),\n",
        "    layer_top\n",
        ")"
      ],
      "metadata": {
        "id": "lpAoYC6Ktw-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Print the summary of your model"
      ],
      "metadata": {
        "id": "g3gSyO12tw-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "print(model4)\n"
      ],
      "metadata": {
        "id": "hrA9Mu6gtw-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.to(device)"
      ],
      "metadata": {
        "id": "PC8acJ9Ctw-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train Neural Networks"
      ],
      "metadata": {
        "id": "_LUet8XQtw-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.1] **TODO** Create 2 variables called `batch_size` and `epochs` that will  respectively take the values 128 and 500"
      ],
      "metadata": {
        "id": "UNYhhHZjtw-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "batch_size = 128\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "PR_k2VuBtw-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.2] **TODO** Compile your model with the appropriate loss function, the optimiser of your choice and the accuracy metric"
      ],
      "metadata": {
        "id": "Gddf5Jsgtw-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lE7ZEAzTtw-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model4.parameters(), lr=0.001, weight_decay = 0.001)"
      ],
      "metadata": {
        "id": "pTUNU-EOC2O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.to(device)"
      ],
      "metadata": {
        "id": "r3ZCtEEctw-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.3] **TODO** Train your model\n",
        "using the number of epochs defined. Calculate the total loss and save it to a variable called total_loss."
      ],
      "metadata": {
        "id": "vRmuC1I3tw-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = epochs\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model4.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for x_batch, y_batch in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model4(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_losses.append(train_loss / len(dataloader_train))\n",
        "\n",
        "    model4.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in dataloader_val:\n",
        "            val_outputs = model4(x_val)\n",
        "            val_loss += criterion(val_outputs, y_val).item()\n",
        "\n",
        "    val_losses.append(val_loss / len(dataloader_val))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "bqNPUGHGtw-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.4] **TODO** Test your model.  Initiate the model.eval() along with torch.no_grad() to turn off the gradients.\n"
      ],
      "metadata": {
        "id": "Byi0L4ngtw-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "\n",
        "model4.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "total_test_loss = 0\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in dataloader_test:\n",
        "        data = data.view(-1, 28*28).to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        if target.ndim > 1:\n",
        "            target = target.argmax(dim=1)\n",
        "\n",
        "        outputs = model4(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        predicted_labels.extend(predicted.cpu().tolist())\n",
        "        true_labels.extend(target.cpu().tolist())\n",
        "\n",
        "avg_test_loss = total_test_loss / len(dataloader_test)\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "chHfTMMEtw-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Analyse Results"
      ],
      "metadata": {
        "id": "CSbXF-4ctw-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.1] **TODO** Display the performance of your model on the training and testing sets"
      ],
      "metadata": {
        "id": "eKUMVzoptw-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_loss = sum(train_losses)\n",
        "final_train_loss = train_losses[-1]\n",
        "\n",
        "total_val_loss = sum(val_losses)\n",
        "final_val_loss = val_losses[-1]\n",
        "\n",
        "print(f\"Total Training Loss: {total_train_loss:.4f}\")\n",
        "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Total Validation Loss: {total_val_loss:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Q9H98VRUuBui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (outputs[0])"
      ],
      "metadata": {
        "id": "ousSjcFLuBuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = np.argmax(outputs.cpu().numpy(), axis=1)\n",
        "pred_test[0]"
      ],
      "metadata": {
        "id": "5qeuRNfTuBuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target[0]"
      ],
      "metadata": {
        "id": "HUeiD_E0uBuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "pEbj0g4-uBuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate_model(model4, dataloader_test, criterion, device, dataset_type=\"test\")\n",
        "val_loss, val_accuracy = evaluate_model(model4, dataloader_val, criterion, device, dataset_type=\"val\")\n",
        "train_loss, train_accuracy = evaluate_model(model4, dataloader_train, criterion, device, dataset_type=\"train\")\n"
      ],
      "metadata": {
        "id": "Ap7lMVyr9dUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.2] **TODO** Plot the learning curve of your model"
      ],
      "metadata": {
        "id": "cLV6AGL_tw-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n",
        "plt.plot(range(1, len(train_losses) + 1), val_losses, label=\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OYOs-7_htw-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.3] **TODO** Display the confusion matrix on the testing set predictions"
      ],
      "metadata": {
        "id": "IVgqn_Qutw-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rkpZeIOFtw-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cLOCw_-bm17"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}